{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3a1f7e",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e4c96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4396e-7bcc-4ccd-bd5c-81f47ae4b4ba",
   "metadata": {},
   "source": [
    "# Select device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e066a3-95ab-4079-b5d3-b39bc2da0676",
   "metadata": {},
   "source": [
    "## Configure device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f08ab8-fb45-4097-b1ca-d47b047d27f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Determine the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c78433",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a0ba5",
   "metadata": {},
   "source": [
    "## Create the custom data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc42706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class FileDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # The number of classes\n",
    "        self.n_classes = 0\n",
    "\n",
    "        # Extract file paths and labels\n",
    "        self._extract_file_paths()\n",
    "        \n",
    "         # Perform label encoding as one-hot encoding\n",
    "        self._encode_labels()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Read the file\n",
    "        data = self._read_file(file_path)\n",
    "\n",
    "        # Apply transformation if provided\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def _extract_file_paths(self):\n",
    "        label_dirs = os.listdir(self.root_dir)\n",
    "\n",
    "        for label in label_dirs:\n",
    "            label_dir = os.path.join(self.root_dir, label)\n",
    "            if os.path.isdir(label_dir):\n",
    "                files = glob.glob(os.path.join(label_dir, '*.csv'))\n",
    "                self.file_list.extend(files)\n",
    "                self.labels.extend([label] * len(files))\n",
    "\n",
    "    def _read_file(self, file_path):\n",
    "        \n",
    "        signal_data = np.loadtxt(file_path, dtype=np.float32, delimiter=\",\")\n",
    "        # Transform to torch vector and reshape to column vector\n",
    "        return torch.from_numpy(signal_data)\n",
    "        \n",
    "        # Implement your own file reading logic here\n",
    "        # For example, if you're working with CSV files, you can use pandas\n",
    "        # dataframe = pd.read_csv(file_path)\n",
    "        # return dataframe.values\n",
    "\n",
    "        # In this example, we assume a simple text file and read its content\n",
    "        #with open(file_path, 'r') as file:\n",
    "        #    content = file.read()\n",
    "\n",
    "        #return content\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(self.labels)\n",
    "        \n",
    "        # Print the original classes names\n",
    "        #print(label_encoder.classes_)\n",
    "        \n",
    "        # Reshape to column vector\n",
    "        integer_encoded = integer_encoded.reshape(-1, 1)\n",
    "\n",
    "        # JCPS \"sparse\" deprecated in version 1.2, use \"sparse_output\" from version 1.4\n",
    "        #onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "        \n",
    "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "        \n",
    "        self.n_classes = onehot_encoded.shape[1]\n",
    "        \n",
    "        self.labels = torch.tensor(onehot_encoded, dtype=torch.float32)\n",
    "        #self.labels = torch.tensor(onehot_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e7bf6",
   "metadata": {},
   "source": [
    "## Define transformations for the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b8881bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reshape the raw data to a row vector\n",
    "\n",
    "class ToRowVector(object):\n",
    "    \"\"\"Transforms the input signal to a row vector\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        #reshaped_sample = sample.view(1, -1)\n",
    "        #print(sample.shape[0])\n",
    "        #print(sample.shape[1])\n",
    "        \n",
    "        # Slide the data, use only the first row\n",
    "        preshaped = sample[0,:]\n",
    "                        \n",
    "        #reshaped_sample = sample.view(sample.shape[0]*sample.shape[1])\n",
    "        reshaped_sample = preshaped.view(preshaped.shape[0])\n",
    "        \n",
    "        #return sample\n",
    "        return reshaped_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07afefbc",
   "metadata": {},
   "source": [
    "## Instantiate the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7afe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: Total number of batches 20 using 5 items per batch. Total samples 100\n",
      "Test loader: Total number of batches 20 using 5 items per batch. Total samples 100\n"
     ]
    }
   ],
   "source": [
    "# The folder with the dataset\n",
    "folder_name = \"../02_python_signal_folder_sorting/sorted_signals_by_mhr\"\n",
    "batch_size = 5\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = FileDataset(folder_name, transform=transforms.Compose([ToRowVector()]))\n",
    "#train_dataset = FileDataset(folder_name)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"Train loader: Total number of batches {} using {} items per batch. Total samples {}\".format(len(train_loader), batch_size, len(train_loader) * batch_size))\n",
    "print(\"Test loader: Total number of batches {} using {} items per batch. Total samples {}\".format(len(test_loader), batch_size, len(test_loader) * batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804d93c",
   "metadata": {},
   "source": [
    "# Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4131b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class BaseClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(BaseClassifier, self).__init__()\n",
    "           \n",
    "        # Define your layers here   \n",
    "        \n",
    "        #self.linear1 = nn.Linear(in_dim, 1024, bias=True)\n",
    "        #self.linear2 = nn.Linear(1024, 256)\n",
    "        #self.relu = nn.ReLU()\n",
    "        #self.linear3 = nn.Linear(256, out_dim)\n",
    "     \n",
    "        self.linear1 = nn.Linear(in_dim, 128, bias=True)\n",
    "        self.linear2 = nn.Linear(128, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(32, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Define the forward pass of your networkloss.item())\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e23f61",
   "metadata": {},
   "source": [
    "# Instantiate the model, optimiser and hyperparameter(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa4936",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5381711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 60000])\n",
      "torch.Size([128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BaseClassifier                           [5, 4]                    --\n",
       "├─Linear: 1-1                            [5, 128]                  7,680,128\n",
       "├─ReLU: 1-2                              [5, 128]                  --\n",
       "├─Linear: 1-3                            [5, 32]                   4,128\n",
       "├─ReLU: 1-4                              [5, 32]                   --\n",
       "├─Linear: 1-5                            [5, 4]                    132\n",
       "==========================================================================================\n",
       "Total params: 7,684,388\n",
       "Trainable params: 7,684,388\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 38.42\n",
       "==========================================================================================\n",
       "Input size (MB): 1.20\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 30.74\n",
       "Estimated Total Size (MB): 31.94\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n_input_features = 2040000\n",
    "n_input_features = 60000 # input dimension\n",
    "n_output_classes = 4 # output dimension\n",
    "\n",
    "# Create an instance of the neural network and move it to the device\n",
    "net = BaseClassifier(n_input_features, n_output_classes).to(device)\n",
    "\n",
    "#net.cuda()\n",
    "#net.cpu()\n",
    "\n",
    "# Define the loss function\n",
    "# Cross-entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Mean Square Error\n",
    "#criterion = nn.MSELoss()\n",
    "\n",
    "# Define the number of epochs\n",
    "n_epochs = 4\n",
    "\n",
    "# Define the optimiser (with its corresponding learning rate)\n",
    "learning_rate = 1e-3\n",
    "# Use Adam optimiser\n",
    "opt = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "#opt = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Summary of the model\n",
    "for p in net.parameters():\n",
    "    print(p.shape)\n",
    "\n",
    "# Summary of the model\n",
    "#summary(net, input_size = (batch_size, 2040000, 4))\n",
    "summary(net, input_size = (batch_size, n_input_features))\n",
    "#summary(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e029e4",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e4c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Batch Loss: 114.2428\n",
      "Epoch [1/4], Batch Loss: 118.9676\n",
      "Epoch [1/4], Batch Loss: 115.5226\n",
      "Epoch [1/4], Batch Loss: 122.5690\n",
      "Epoch [1/4], Batch Loss: 49.0515\n",
      "Epoch [1/4], Batch Loss: 121.3692\n",
      "Epoch [1/4], Batch Loss: 157.7504\n",
      "Epoch [1/4], Batch Loss: 377.1904\n",
      "Epoch [1/4], Batch Loss: 194.2703\n",
      "Epoch [1/4], Batch Loss: 543.5787\n",
      "Epoch [1/4], Batch Loss: 116.6120\n",
      "Epoch [1/4], Batch Loss: 454.6817\n",
      "Epoch [1/4], Batch Loss: 556.0588\n",
      "Epoch [1/4], Batch Loss: 23.7612\n",
      "Epoch [1/4], Batch Loss: 903.6849\n",
      "Epoch [1/4], Batch Loss: 236.4304\n",
      "Epoch [1/4], Batch Loss: 257.3744\n",
      "Epoch [1/4], Batch Loss: 74.5156\n",
      "Epoch [1/4], Batch Loss: 658.7189\n",
      "Epoch [1/4], Batch Loss: 149.1948\n",
      "Epoch: 1 training loss: 267.2773\n",
      "Epoch [2/4], Batch Loss: 303.6268\n",
      "Epoch [2/4], Batch Loss: 476.2717\n",
      "Epoch [2/4], Batch Loss: 166.1959\n",
      "Epoch [2/4], Batch Loss: 94.5315\n",
      "Epoch [2/4], Batch Loss: 162.5549\n",
      "Epoch [2/4], Batch Loss: -0.0000\n",
      "Epoch [2/4], Batch Loss: 192.0401\n",
      "Epoch [2/4], Batch Loss: 1082.8665\n",
      "Epoch [2/4], Batch Loss: -0.0000\n",
      "Epoch [2/4], Batch Loss: 156.3266\n",
      "Epoch [2/4], Batch Loss: 953.3527\n",
      "Epoch [2/4], Batch Loss: 371.0950\n",
      "Epoch [2/4], Batch Loss: 261.3499\n",
      "Epoch [2/4], Batch Loss: 235.0586\n",
      "Epoch [2/4], Batch Loss: -0.0000\n",
      "Epoch [2/4], Batch Loss: 205.8611\n",
      "Epoch [2/4], Batch Loss: 474.2918\n",
      "Epoch [2/4], Batch Loss: 546.8434\n",
      "Epoch [2/4], Batch Loss: 61.6461\n",
      "Epoch [2/4], Batch Loss: 326.1691\n",
      "Epoch: 2 training loss: 303.5041\n",
      "Epoch [3/4], Batch Loss: -0.0000\n",
      "Epoch [3/4], Batch Loss: 120.0573\n",
      "Epoch [3/4], Batch Loss: -0.0000\n",
      "Epoch [3/4], Batch Loss: 11.7485\n",
      "Epoch [3/4], Batch Loss: -0.0000\n",
      "Epoch [3/4], Batch Loss: -0.0000\n",
      "Epoch [3/4], Batch Loss: -0.0000\n",
      "Epoch [3/4], Batch Loss: -0.0000\n",
      "Epoch [3/4], Batch Loss: 63.0718\n",
      "Epoch [3/4], Batch Loss: -0.0000\n",
      "Epoch [3/4], Batch Loss: -0.0000\n",
      "Epoch [3/4], Batch Loss: 619.9596\n",
      "Epoch [3/4], Batch Loss: -0.0000\n",
      "Epoch [3/4], Batch Loss: 262.0772\n",
      "Epoch [3/4], Batch Loss: 116.0485\n",
      "Epoch [3/4], Batch Loss: 12.6870\n",
      "Epoch [3/4], Batch Loss: 115.4396\n",
      "Epoch [3/4], Batch Loss: -0.0000\n",
      "Epoch [3/4], Batch Loss: -0.0000\n",
      "Epoch [3/4], Batch Loss: 80.2073\n",
      "Epoch: 3 training loss: 70.0648\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: 93.4526\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: 0.0213\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: 17.6462\n",
      "Epoch [4/4], Batch Loss: -0.0000\n",
      "Epoch [4/4], Batch Loss: -0.0000\n"
     ]
    }
   ],
   "source": [
    "# Define the training method\n",
    "def train(model=net,\n",
    "          optimizer=opt,\n",
    "          n_epochs=n_epochs,\n",
    "          loss_fn=criterion,\n",
    "          lr=learning_rate):\n",
    "    \n",
    "    # Indicate the Pytorch backend we are on training mode\n",
    "    model.train()\n",
    "    loss_lt = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "        \n",
    "            # Prior any operation clear the gradient\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # Move data and labels to the device\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "        \n",
    "            # Forward pass\n",
    "            outputs = model(batch_data)\n",
    "\n",
    "            #print(\"Outputs\\n\")\n",
    "            #print(outputs)\n",
    "        \n",
    "            #print(\"Batch labels\\n\")\n",
    "            #print(batch_labels)\n",
    "        \n",
    "            # Apply the loss function\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "        \n",
    "            # Perform an optimization step (this updates the weights and bias on the network)\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # Keep track of sum of loss of each batch\n",
    "            running_loss+=loss.item()\n",
    "                \n",
    "            # Print the loss for monitoring\n",
    "            print('Epoch [{}/{}], Batch Loss: {:.4f}'.format(epoch+1, n_epochs, loss.item()))\n",
    "        \n",
    "        # Add the cumulative loss to a list\n",
    "        loss_lt.append(running_loss/len(train_loader))\n",
    "        \n",
    "        # Print the total loss of the epoch\n",
    "        print('Epoch: {} training loss: {:.4f}'.format(epoch+1, running_loss/len(train_loader)))\n",
    "        \n",
    "    plt.plot([i for i in range(1, n_epochs+1)], loss_lt, label=\"Train\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Cross-entropy\")\n",
    "    plt.title(\"Training loss: optimiser {}, lr {:.6f}\".format(\"Adam\", lr))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(net.state_dict(), \"./trained_model/model.pt')\n",
    "\n",
    "# Call the training method\n",
    "train(net, opt, n_epochs, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30015c65",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2673a35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test batch loss: -0.0000, test batch accuracy: 100.0000\n",
      "Test batch loss: -0.0000, test batch accuracy: 100.0000\n",
      "Test batch loss: -0.0000, test batch accuracy: 100.0000\n",
      "Test batch loss: 0.2313, test batch accuracy: 96.0000\n",
      "Test loss: 0.0578, test accuracy: 99.0000\n"
     ]
    }
   ],
   "source": [
    "# Define a helper function to generate a one-hot encoding at the position of the maximum value\n",
    "def generate_max_indices_tensor(input_tensor):\n",
    "    # Compute the maximum along each row\n",
    "    max_values, max_indices = torch.max(input_tensor, dim=1)\n",
    "    \n",
    "    # Create a tensor of zeros with the same shape as the input tensor\n",
    "    output_tensor = torch.zeros_like(input_tensor)\n",
    "    \n",
    "    # Set ones at the indices of the maximum values\n",
    "    output_tensor.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "    \n",
    "    return output_tensor\n",
    "\n",
    "# Define a helper function that returns a one tensor if the input tensors are equal\n",
    "def compare_tensors(tensor1, tensor2):\n",
    "    \n",
    "    # Ensure both tensors have the same shape\n",
    "    assert tensor1.shape == tensor2.shape, \"Both tensors should have the same dimensions.\"\n",
    "\n",
    "    # Calculate element-wise equality and count equal rows\n",
    "    row_equality = torch.all(tensor1 == tensor2, dim=1)\n",
    "    equal_rows = torch.sum(row_equality).item()\n",
    "\n",
    "    # Count different rows\n",
    "    different_rows = tensor1.shape[0] - equal_rows\n",
    "    \n",
    "    return torch.tensor([equal_rows, different_rows])\n",
    "        \n",
    "# Define the testing method\n",
    "def test(model=net,\n",
    "        loss_fn=criterion):\n",
    "    \n",
    "    # Indicate the Pytorch backend we are on testing mode\n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    batch_loss = 0.0\n",
    "    batch_accuracy = 0.0\n",
    "    \n",
    "    # Use no grad to reduce memory and computation cost\n",
    "    with torch.no_grad():\n",
    "                \n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            \n",
    "            # Move data and labels to the device\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_data)\n",
    "            \n",
    "            #print(\"These are the outputs\")\n",
    "            #print(outputs)\n",
    "            \n",
    "            #print(\"These are the batch labels\")\n",
    "            #print(batch_labels)\n",
    "            \n",
    "            # Compute the loss\n",
    "            batch_loss = loss_fn(outputs, batch_labels)\n",
    "            \n",
    "            # Add up the loss\n",
    "            total_loss+=batch_loss.item()\n",
    "            \n",
    "            # Compute the one-hot enconding version\n",
    "            one_hot_output = generate_max_indices_tensor(outputs)\n",
    "            \n",
    "            #print(\"These are the one-hot outputs\")\n",
    "            #print(one_hot_output)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            batch_accuracy = compare_tensors(one_hot_output, batch_labels)[0]\n",
    "            #batch_accuracy = torch.sum(compare_tensors(one_hot_output, batch_labels))\n",
    "            #print(compare_tensors(one_hot_output, batch_labels))\n",
    "            \n",
    "            accuracy+=batch_accuracy\n",
    "            \n",
    "            print(\"Test batch loss: {:.4f}, test batch accuracy: {:.4f}\".format(\n",
    "            batch_loss/batch_size,\n",
    "            batch_accuracy*100.0/batch_size))\n",
    "            \n",
    "        print(\"Test loss: {:.4f}, test accuracy: {:.4f}\".format(\n",
    "            total_loss/(len(test_loader)*batch_size),\n",
    "            accuracy*100.0/(len(test_loader)*batch_size)))\n",
    "\n",
    "# Call the test method\n",
    "test(net, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fae8d6",
   "metadata": {},
   "source": [
    "# Code testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_data, batch_labels in train_loader:\n",
    "    print(batch_data.shape)\n",
    "    print(batch_labels)\n",
    "    print(batch_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = np.loadtxt(\"../02_python_signal_folder_sorting/sorted_signals_by_mhr/70_74/nifecg.0003.fs_1000_mhr_72_fhr_132.csv\", dtype=np.float32, delimiter=\",\")\n",
    "signal_data_torch = torch.from_numpy(signal_data)\n",
    "signal_data_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ff5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-hot enconding\n",
    "encoder = OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "print(x)\n",
    "idx, x_max = x.max(dim=1)\n",
    "print(x_max)\n",
    "x_arg_max = torch.argmax(x, 1)\n",
    "print(x_arg_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ed8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4fee4-589a-4608-bb67-0d91423ad5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
