{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3a1f7e",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from torchinfo import summary\n",
    "import torchmetrics\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4396e-7bcc-4ccd-bd5c-81f47ae4b4ba",
   "metadata": {},
   "source": [
    "# Select device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e066a3-95ab-4079-b5d3-b39bc2da0676",
   "metadata": {},
   "source": [
    "## Configure device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f08ab8-fb45-4097-b1ca-d47b047d27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa79784",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0378ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder with the dataset\n",
    "#training_folder_name = \"../../data/sorted_by_mhr/training_set\"\n",
    "#testing_folder_name = \"../../data/sorted_by_mhr/testing_set\"\n",
    "\n",
    "# The folder with the dataset\n",
    "#training_folder_name = \"../02_python_signal_folder_sorting/signals_by_range_and_mhr/training_set\"\n",
    "#testing_folder_name = \"../02_python_signal_folder_sorting/signals_by_range_and_mhr/testing_set\"\n",
    "training_folder_name = \"../../data/sorted_by_fhr/training_set\"\n",
    "testing_folder_name = \"../../data/sorted_by_fhr/testing_set\"\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "# Specify the channels to work with\n",
    "#channels = [0] # one channel\n",
    "channels = [0, 4, 8, 12, 16, 20, 24, 28, 32] # eight channels\n",
    "\n",
    "# Specify the sample interval\n",
    "sample_interval = 10\n",
    "#sample_interval = 1\n",
    "\n",
    "# Define the number of epochs\n",
    "n_epochs = 3\n",
    "\n",
    "# Define the optimiser (with its corresponding learning rate)\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# The number of output clasess\n",
    "#n_output_classes = 4 # output dimension\n",
    "#n_output_classes = 11 # output dimension\n",
    "n_output_classes = 6 # output dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c78433",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a0ba5",
   "metadata": {},
   "source": [
    "## Create the custom data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc42706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class FileDataset(Dataset):\n",
    "    def __init__(self, root_dir, channels, sample_interval, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.sample_interval = sample_interval\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # The number of classes\n",
    "        self.n_classes = 0\n",
    "\n",
    "        # Extract file paths and labels\n",
    "        self._extract_file_paths()\n",
    "        \n",
    "         # Perform label encoding as one-hot encoding\n",
    "        self._encode_labels()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Read the file\n",
    "        data = self._read_file(file_path)\n",
    "\n",
    "        # Apply transformation if provided\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def _extract_file_paths(self):\n",
    "        label_dirs = os.listdir(self.root_dir)\n",
    "\n",
    "        for label in label_dirs:\n",
    "            label_dir = os.path.join(self.root_dir, label)\n",
    "            if os.path.isdir(label_dir):\n",
    "                files = glob.glob(os.path.join(label_dir, '*.csv'))\n",
    "                self.file_list.extend(files)\n",
    "                self.labels.extend([label] * len(files))\n",
    "\n",
    "    def _read_file(self, file_path):\n",
    "        \n",
    "        signal_data = np.loadtxt(file_path, dtype=np.float32, delimiter=\",\")\n",
    "        # Transform to torch vector and reshape to column vector\n",
    "        return torch.from_numpy(signal_data)\n",
    "        \n",
    "        # Implement your own file reading logic here\n",
    "        # For example, if you're working with CSV files, you can use pandas\n",
    "        # dataframe = pd.read_csv(file_path)\n",
    "        # return dataframe.values\n",
    "\n",
    "        # In this example, we assume a simple text file and read its content\n",
    "        #with open(file_path, 'r') as file:\n",
    "        #    content = file.read()\n",
    "\n",
    "        #return content\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(self.labels)\n",
    "        \n",
    "        # Print the original classes names\n",
    "        #print(label_encoder.classes_)\n",
    "        \n",
    "        # Reshape to column vector\n",
    "        integer_encoded = integer_encoded.reshape(-1, 1)\n",
    "\n",
    "        # JCPS \"sparse\" deprecated in version 1.2, use \"sparse_output\" from version 1.4\n",
    "        #onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "        \n",
    "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "        \n",
    "        self.n_classes = onehot_encoded.shape[1]\n",
    "        \n",
    "        self.labels = torch.tensor(onehot_encoded, dtype=torch.float32)\n",
    "        #self.labels = torch.tensor(onehot_encoded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e7bf6",
   "metadata": {},
   "source": [
    "## Define transformations for the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8881bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the raw data to a row vector\n",
    "\n",
    "class ToRowVector(object):\n",
    "    \"\"\"Transforms the input signal to a row vector\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        #reshaped_sample = sample.view(1, -1)\n",
    "        #print(sample.shape[0])\n",
    "        #print(sample.shape[1])\n",
    "        \n",
    "        # Slide the data, use only the first row\n",
    "        preshaped = sample[0,:]\n",
    "                        \n",
    "        #reshaped_sample = sample.view(sample.shape[0]*sample.shape[1])\n",
    "        reshaped_sample = preshaped.view(preshaped.shape[0])\n",
    "        \n",
    "        #return sample\n",
    "        return reshaped_sample\n",
    "\n",
    "# Subsample signal extracting only selected channels and gor a given sample interval\n",
    "class SignalSubSample(object):\n",
    "    \n",
    "    def __init__(self, channels, sample_interval):\n",
    "        \"\"\"\n",
    "        Initialize input arguments.\n",
    "        \"\"\"\n",
    "        self.channels = channels\n",
    "        self.sample_interval = sample_interval\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "            \n",
    "        # Get the indices of the columns to extract\n",
    "        indexes_columns = list(range(0, sample.shape[1], sample_interval))  # Select every \"sample_interval\" column\n",
    "        \n",
    "        # Extract selected channels and rows\n",
    "        subsample_signal = sample[channels, :][:, indexes_columns]                \n",
    "        \n",
    "        #print(subsample_signal.shape)\n",
    "                               \n",
    "        #return sample\n",
    "        return subsample_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07afefbc",
   "metadata": {},
   "source": [
    "## Instantiate the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7afe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training datasets\n",
    "training_dataset = FileDataset(training_folder_name, channels, \n",
    "                            sample_interval, \n",
    "                            transform=transforms.Compose([\n",
    "                                SignalSubSample(channels, sample_interval)\n",
    "                            ]))\n",
    "\n",
    "# Create testing datasets\n",
    "testing_dataset = FileDataset(testing_folder_name, channels, \n",
    "                            sample_interval, \n",
    "                            transform=transforms.Compose([\n",
    "                                SignalSubSample(channels, sample_interval)\n",
    "                            ]))\n",
    "\n",
    "#train_dataset = FileDataset(folder_name, channels, \n",
    "#                            sample_interval, \n",
    "#                            transform=transforms.Compose([\n",
    "#                                ToRowVector()\n",
    "#                            ]))\n",
    "#train_dataset = FileDataset(folder_name)\n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"Train loader: Total number of batches {} using {} items per batch. Total samples {}\".format(len(train_loader), batch_size, len(train_loader) * batch_size))\n",
    "print(\"Test loader: Total number of batches {} using {} items per batch. Total samples {}\".format(len(test_loader), batch_size, len(test_loader) * batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804d93c",
   "metadata": {},
   "source": [
    "# Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class BaseClassifier(nn.Module):\n",
    "    def __init__(self, n_input_data_x, n_input_data_y, n_output):\n",
    "        super(BaseClassifier, self).__init__()\n",
    "           \n",
    "        # Define your layers here   \n",
    "        \n",
    "        #self.linear1 = nn.Linear(in_dim, 1024, bias=True)\n",
    "        #self.linear2 = nn.Linear(1024, 256)\n",
    "        #self.relu = nn.ReLU()\n",
    "        #self.linear3 = nn.Linear(256, out_dim)\n",
    "     \n",
    "        input_features = n_input_data_x * n_input_data_y\n",
    "    \n",
    "        self.linear1 = nn.Linear(input_features, 128, bias=True)\n",
    "        self.linear2 = nn.Linear(128, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(32, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"Before the flattening\")\n",
    "        #print(x.shape[0])\n",
    "        #print(x.shape[1])\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Perform the flattening of the input data to pass through the linear layers\n",
    "        #reshaped_sample = sample.view(sample.shape[0]*sample.shape[1])\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        #print(\"After the flattening\")\n",
    "        #print(x.shape[0])\n",
    "        #print(x.shape[1])\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Define the forward pass of your networkloss.item())\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e23f61",
   "metadata": {},
   "source": [
    "# Instantiate the optimiser and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa4936",
   "metadata": {},
   "source": [
    "## Optimiser and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5381711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of channels on the processing signal\n",
    "n_channels = len(channels)\n",
    "\n",
    "# Original number of data per channel\n",
    "n_original_data_per_channel = 60000\n",
    "\n",
    "# Get the number of data per channel\n",
    "n_data_per_channel = n_original_data_per_channel // sample_interval # Integer division\n",
    "\n",
    "#n_input_features = 2040000\n",
    "n_input_features = n_channels * n_data_per_channel # input dimension\n",
    "print(\"Number of input feautres: {}\\n\".format(n_input_features))\n",
    "\n",
    "# Create an instance of the neural network and move it to the device\n",
    "net = BaseClassifier(n_channels, n_data_per_channel, n_output_classes).to(device)\n",
    "\n",
    "#net.cuda()\n",
    "#net.cpu()\n",
    "\n",
    "# Define the loss function\n",
    "# Cross-entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Mean Square Error\n",
    "#criterion = nn.MSELoss()\n",
    "\n",
    "# Use Adam optimiser\n",
    "opt = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "#opt = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Summary of the model\n",
    "for p in net.parameters():\n",
    "    print(p.shape)\n",
    "\n",
    "# Summary of the model\n",
    "#summary(net, input_size = (batch_size, 2040000, 4))\n",
    "summary(net, input_size = (batch_size, n_input_features))\n",
    "#summary(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd374f04",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to generate a one-hot encoding at the position of the maximum value\n",
    "def generate_max_indices_tensor(input_tensor):\n",
    "    # Compute the maximum along each row\n",
    "    max_values, max_indices = torch.max(input_tensor, dim=1)\n",
    "    \n",
    "    # Create a tensor of zeros with the same shape as the input tensor\n",
    "    output_tensor = torch.zeros_like(input_tensor)\n",
    "    \n",
    "    # Set ones at the indices of the maximum values\n",
    "    output_tensor.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "    \n",
    "    return output_tensor\n",
    "\n",
    "# Define a helper function that returns a one tensor if the input tensors are equal\n",
    "def compare_tensors(tensor1, tensor2):\n",
    "    \n",
    "    # Ensure both tensors have the same shape\n",
    "    assert tensor1.shape == tensor2.shape, \"Both tensors should have the same dimensions.\"\n",
    "\n",
    "    # Calculate element-wise equality and count equal rows\n",
    "    row_equality = torch.all(tensor1 == tensor2, dim=1)\n",
    "    equal_rows = torch.sum(row_equality).item()\n",
    "\n",
    "    # Count different rows\n",
    "    different_rows = tensor1.shape[0] - equal_rows\n",
    "    \n",
    "    return torch.tensor([equal_rows, different_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e029e4",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training method\n",
    "def train(model=net,\n",
    "          optimizer=opt,\n",
    "          n_epochs=n_epochs,\n",
    "          loss_fn=criterion,\n",
    "          lr=learning_rate):\n",
    "    \n",
    "    accuracy_metric = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes = n_output_classes)\n",
    "    \n",
    "    # Indicate the Pytorch backend we are on training mode\n",
    "    model.train()\n",
    "    loss_lt = []\n",
    "    accuracy_lt = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        accuracy_epoch = 0.0\n",
    "        batch_counter = 0\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "        \n",
    "            # Prior any operation clear the gradient\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # Move data and labels to the device\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "        \n",
    "            # Forward pass\n",
    "            outputs = model(batch_data)\n",
    "\n",
    "            #print(\"Outputs\\n\")\n",
    "            #print(outputs)\n",
    "        \n",
    "            #print(\"Batch labels\\n\")\n",
    "            #print(batch_labels)\n",
    "        \n",
    "            # Apply the loss function\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "        \n",
    "            # Perform an optimization step (this updates the weights and bias on the network)\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # Keep track of sum of loss of each batch\n",
    "            running_loss+=loss.item()\n",
    "            \n",
    "             # Compute the one-hot enconding version\n",
    "            one_hot_output = generate_max_indices_tensor(outputs)\n",
    "                   \n",
    "            # Compute accuracy\n",
    "            batch_accuracy = compare_tensors(one_hot_output, batch_labels)[0]\n",
    "            \n",
    "            # Keep track of accuracy\n",
    "            accuracy_epoch+=batch_accuracy\n",
    "            \n",
    "            # Metrics\n",
    "            acc = accuracy_metric(outputs, batch_labels)\n",
    "            \n",
    "            # Print the loss for monitoring\n",
    "            print('Epoch [{}/{}], batch [{}/{}], lr={:.6f}, batch Loss: {:.4f}, batch accuracy: {:.4f}'.format(epoch+1, n_epochs,\n",
    "                                                                                                                batch_counter + 1, len(train_loader),\n",
    "                                                                                                                lr, loss.item(),\n",
    "                                                                                                                batch_accuracy*100.0/batch_size))\n",
    "            print(\"Torch Metrics: {}\".format(acc))\n",
    "            \n",
    "            # Increase the batch counter\n",
    "            batch_counter += 1\n",
    "        \n",
    "        # Add the cumulative loss to a list\n",
    "        loss_lt.append(running_loss/len(train_loader))\n",
    "        \n",
    "        # Add the cumulative accuracy to a list\n",
    "        accuracy_lt.append(accuracy_epoch/len(train_loader)) \n",
    "        \n",
    "        # Print the total loss of the epoch\n",
    "        print('Epoch: {} training loss: {:.4f}, training accuracy: {:.4f}'.format(epoch+1, running_loss/len(train_loader), accuracy_epoch/len(train_loader)))\n",
    "        \n",
    "        acc = accuracy_metric.compute()\n",
    "        print(\"All batches torch metric: {}\".format(acc))\n",
    "    \n",
    "    accuracy_metric.reset()\n",
    "    \n",
    "    fig = plt.figure(figsize=(17, 5))\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax1.plot([i for i in range(1, n_epochs+1)], loss_lt, label=\"Loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Cross-entropy\")\n",
    "    ax1.set_title(\"Training loss: optimiser {}, lr {:.6f}\".format(\"Adam\", lr))\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2 = plt.subplot(122)\n",
    "    ax2.plot([i for i in range(1, n_epochs+1)], accuracy_lt, label=\"Accuracy\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_title(\"Training accuracy: optimiser {}, lr {:.6f}\".format(\"Adam\", lr))\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(net.state_dict(), \"./trained_model/model.pt')\n",
    "\n",
    "# Call the training method\n",
    "train(net, opt, n_epochs, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30015c65",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing method\n",
    "def test(model=net,\n",
    "        loss_fn=criterion,\n",
    "        lr=learning_rate):\n",
    "    \n",
    "    # Indicate the Pytorch backend we are on testing mode\n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total_loss = 0.0 \n",
    "    \n",
    "    # Use no grad to reduce memory and computation cost\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        batch_counter = 0\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            \n",
    "            # Move data and labels to the device\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_data)\n",
    "            \n",
    "            #print(\"These are the outputs\")\n",
    "            #print(outputs)\n",
    "            \n",
    "            #print(\"These are the batch labels\")\n",
    "            #print(batch_labels)\n",
    "            \n",
    "            # Compute the loss\n",
    "            batch_loss = loss_fn(outputs, batch_labels)\n",
    "            \n",
    "            # Add up the loss\n",
    "            total_loss+=batch_loss.item()\n",
    "            \n",
    "            # Compute the one-hot enconding version\n",
    "            one_hot_output = generate_max_indices_tensor(outputs)\n",
    "            \n",
    "            #print(\"These are the one-hot outputs\")\n",
    "            #print(one_hot_output)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            batch_accuracy = compare_tensors(one_hot_output, batch_labels)[0]\n",
    "            #batch_accuracy = torch.sum(compare_tensors(one_hot_output, batch_labels))\n",
    "            #print(compare_tensors(one_hot_output, batch_labels))\n",
    "            \n",
    "            accuracy+=batch_accuracy\n",
    "            \n",
    "            print(\"Test batch [{}/{}], lr={:.6f},  batch loss: {:.4f}, test batch accuracy: {:.4f}\".format(\n",
    "                batch_counter + 1, len(test_loader), lr, \n",
    "                batch_loss/batch_size,\n",
    "                batch_accuracy*100.0/batch_size))\n",
    "            \n",
    "            # Increase the batch counter\n",
    "            batch_counter += 1\n",
    "            \n",
    "        print(\"Test loss: {:.4f}, test accuracy: {:.4f}\".format(\n",
    "            total_loss/(len(test_loader)*batch_size),\n",
    "            accuracy*100.0/(len(test_loader)*batch_size)))\n",
    "\n",
    "# Call the test method\n",
    "test(net, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fae8d6",
   "metadata": {},
   "source": [
    "# Code testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_data, batch_labels in train_loader:\n",
    "    print(batch_data.shape)\n",
    "    print(batch_labels)\n",
    "    print(batch_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = np.loadtxt(\"../02_python_signal_folder_sorting/sorted_signals_by_mhr/70_74/nifecg.0003.fs_1000_mhr_72_fhr_132.csv\", dtype=np.float32, delimiter=\",\")\n",
    "signal_data_torch = torch.from_numpy(signal_data)\n",
    "signal_data_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ff5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-hot enconding\n",
    "encoder = OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "print(x)\n",
    "idx, x_max = x.max(dim=1)\n",
    "print(x_max)\n",
    "x_arg_max = torch.argmax(x, 1)\n",
    "print(x_arg_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ed8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4fee4-589a-4608-bb67-0d91423ad5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
