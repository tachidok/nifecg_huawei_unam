{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3a1f7e",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e4c96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from torchinfo import summary\n",
    "#import torchmetrics\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4396e-7bcc-4ccd-bd5c-81f47ae4b4ba",
   "metadata": {},
   "source": [
    "# Select device (CPU or GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e066a3-95ab-4079-b5d3-b39bc2da0676",
   "metadata": {},
   "source": [
    "## Configure device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f08ab8-fb45-4097-b1ca-d47b047d27f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Determine the device (NPU, CPU or GPU)\n",
    "device = \"cpu\"\n",
    "if torch.npu.is_available():\n",
    "    device = \"npu:0\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "torch.device(device)\n",
    "'''\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa79784",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0378ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder with the dataset\n",
    "#training_folder_name = \"../../data/sorted_by_mhr/training_set\"\n",
    "#testing_folder_name = \"../../data/sorted_by_mhr/testing_set\"\n",
    "\n",
    "# The folder with the dataset\n",
    "#training_folder_name = \"../02_python_signal_folder_sorting/signals_by_range_and_mhr/training_set\"\n",
    "#testing_folder_name = \"../02_python_signal_folder_sorting/signals_by_range_and_mhr/testing_set\"\n",
    "training_folder_name = \"../../data/sorted_by_fhr/training_set\"\n",
    "testing_folder_name = \"../../data/sorted_by_fhr/testing_set\"\n",
    "\n",
    "#batch_size = 10\n",
    "batch_size = 25\n",
    "\n",
    "# Specify the channels to work with\n",
    "#channels = [0] # one channel\n",
    "#channels = [0, 4, 8, 12, 16, 20, 24, 28, 32] # eight channels\n",
    "channels = [0, 4] # eight channels\n",
    "\n",
    "# Specify the sample interval (we have 1 minute signal with 60,000 points *60Hz*)\n",
    "sample_interval = 10 # reduce the number of samples to 6,000\n",
    "#sample_interval = 1 # use the whole signal samples\n",
    "\n",
    "# Define the number of epochs\n",
    "n_epochs = 1\n",
    "\n",
    "# Define the optimiser (with its corresponding learning rate)\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# The number of output clasess\n",
    "#n_output_classes = 4 # output dimension\n",
    "#n_output_classes = 11 # output dimension\n",
    "n_output_classes = 6 # output dimension (this is incorrect but works because in the laptop we have only six groups)\n",
    "#n_output_classes = 7 # output dimension uncomment this line when move to UNAM server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c78433",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a0ba5",
   "metadata": {},
   "source": [
    "## Create the custom data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ecc42706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class FileDataset(Dataset):\n",
    "    def __init__(self, root_dir, channels, sample_interval, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.sample_interval = sample_interval\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # The number of classes\n",
    "        self.n_classes = 0\n",
    "\n",
    "        # Extract file paths and labels\n",
    "        self._extract_file_paths()\n",
    "        \n",
    "         # Perform label encoding as one-hot encoding\n",
    "        self._encode_labels()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Read the file\n",
    "        data = self._read_file(file_path)\n",
    "\n",
    "        # Apply transformation if provided\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return data, label\n",
    "\n",
    "    def _extract_file_paths(self):\n",
    "        label_dirs = os.listdir(self.root_dir)\n",
    "\n",
    "        for label in label_dirs:\n",
    "            label_dir = os.path.join(self.root_dir, label)\n",
    "            if os.path.isdir(label_dir):\n",
    "                files = glob.glob(os.path.join(label_dir, '*.csv'))\n",
    "                self.file_list.extend(files)\n",
    "                self.labels.extend([label] * len(files))\n",
    "\n",
    "    def _read_file(self, file_path):\n",
    "        \n",
    "        signal_data = np.loadtxt(file_path, dtype=np.float32, delimiter=\",\")\n",
    "        # Transform to torch vector and reshape to column vector\n",
    "        return torch.from_numpy(signal_data)\n",
    "        \n",
    "        # Implement your own file reading logic here\n",
    "        # For example, if you're working with CSV files, you can use pandas\n",
    "        # dataframe = pd.read_csv(file_path)\n",
    "        # return dataframe.values\n",
    "\n",
    "        # In this example, we assume a simple text file and read its content\n",
    "        #with open(file_path, 'r') as file:\n",
    "        #    content = file.read()\n",
    "\n",
    "        #return content\n",
    "        \n",
    "    def _encode_labels(self):\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(self.labels)\n",
    "        \n",
    "        # Print the original classes names\n",
    "        #print(label_encoder.classes_)\n",
    "        \n",
    "        # Reshape to column vector\n",
    "        integer_encoded = integer_encoded.reshape(-1, 1)\n",
    "\n",
    "        # JCPS \"sparse\" deprecated in version 1.2, use \"sparse_output\" from version 1.4\n",
    "        #onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "        \n",
    "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "        \n",
    "        self.n_classes = onehot_encoded.shape[1]\n",
    "        \n",
    "        self.labels = torch.tensor(onehot_encoded, dtype=torch.float32)\n",
    "        #self.labels = torch.tensor(onehot_encoded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e7bf6",
   "metadata": {},
   "source": [
    "## Define transformations for the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b8881bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the raw data to a row vector\n",
    "\n",
    "class ToRowVector(object):\n",
    "    \"\"\"Transforms the input signal to a row vector\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        #reshaped_sample = sample.view(1, -t1)\n",
    "        #print(sample.shape[0])\n",
    "        #print(sample.shape[1])\n",
    "        \n",
    "        # Slide the data, use only the first row\n",
    "        preshaped = sample[0,:]\n",
    "                        \n",
    "        #reshaped_sample = sample.view(sample.shape[0]*sample.shape[1])\n",
    "        reshaped_sample = preshaped.view(preshaped.shape[0])\n",
    "        \n",
    "        #return sample\n",
    "        return reshaped_sample\n",
    "\n",
    "# Subsample signal extracting only selected channels and/or a given sample interval\n",
    "class SignalSubSample(object):\n",
    "    \n",
    "    def __init__(self, channels, sample_interval):\n",
    "        \"\"\"\n",
    "        Initialize input arguments.\n",
    "        \"\"\"\n",
    "        self.channels = channels\n",
    "        self.sample_interval = sample_interval\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "            \n",
    "        # Get the indices of the columns to extract\n",
    "        indexes_columns = list(range(0, sample.shape[1], sample_interval))  # Select every \"sample_interval\" column\n",
    "        \n",
    "        # Extract selected channels and columns\n",
    "        subsample_signal = sample[channels, :][:, indexes_columns]\n",
    "        \n",
    "        #print(subsample_signal.shape)\n",
    "                               \n",
    "        #return sample\n",
    "        return subsample_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07afefbc",
   "metadata": {},
   "source": [
    "## Instantiate the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af7afe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: Total number of batches 44 using 25 items per batch. Total samples 1100\n",
      "Test loader: Total number of batches 11 using 25 items per batch. Total samples 275\n"
     ]
    }
   ],
   "source": [
    "# Create training datasets\n",
    "training_dataset = FileDataset(training_folder_name, channels, \n",
    "                            sample_interval, \n",
    "                            transform=transforms.Compose([\n",
    "                                SignalSubSample(channels, sample_interval)#,\n",
    "                                #transforms.ToTensor() this does not apply because our input data are not images\n",
    "                                #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                #transforms.Normalize((0.1), (0.1)) # We need a method to normalize a tensor, not an image\n",
    "                            ]))\n",
    "\n",
    "# Create testing datasets\n",
    "testing_dataset = FileDataset(testing_folder_name, channels, \n",
    "                            sample_interval, \n",
    "                            transform=transforms.Compose([\n",
    "                                SignalSubSample(channels, sample_interval)#,\n",
    "                                #transforms.ToTensor() this does not apply because our input data are not images\n",
    "                                #transforms.Normalize(mean=[0.1], std=[0.01])\n",
    "                            ]))\n",
    "\n",
    "#train_dataset = FileDataset(folder_name, channels, \n",
    "#                            sample_interval, \n",
    "#                            transform=transforms.Compose([\n",
    "#                                ToRowVector()\n",
    "#                            ]))\n",
    "#train_dataset = FileDataset(folder_name)\n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"Train loader: Total number of batches {} using {} items per batch. Total samples {}\".format(len(train_loader), batch_size, len(train_loader) * batch_size))\n",
    "print(\"Test loader: Total number of batches {} using {} items per batch. Total samples {}\".format(len(test_loader), batch_size, len(test_loader) * batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804d93c",
   "metadata": {},
   "source": [
    "# Define the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74807fad",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4131b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class LinearClassifier(nn.Module):\n",
    "    # n_input_data_dim1: Number of data in dimension one (number of rows/channels)\n",
    "    # n_input_data_dim2: Number of data in dimension two (number of columns/data_per_channel)\n",
    "    def __init__(self, n_input_data_dim1, n_input_data_dim2, n_output):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "           \n",
    "        # Define your layers here   \n",
    "        \n",
    "        #self.linear1 = nn.Linear(in_dim, 1024, bias=True)\n",
    "        #self.linear2 = nn.Linear(1024, 256)\n",
    "        #self.relu = nn.ReLU()\n",
    "        #self.linear3 = nn.Linear(256, out_dim)\n",
    "     \n",
    "        input_features = n_input_data_dim1 * n_input_data_dim2\n",
    "    \n",
    "        self.linear1 = nn.Linear(input_features, 128, bias=True)\n",
    "        self.linear2 = nn.Linear(128, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(32, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"Before the flattening\")\n",
    "        #print(x.shape[0])\n",
    "        #print(x.shape[1])\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Perform the flattening of the input data to pass through the linear layers\n",
    "        #reshaped_sample = sample.view(sample.shape[0]*sample.shape[1])\n",
    "    #accuracy_metric = torchmetrics.classification.Accuracy(task=\"multic\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        #print(\"After the flattening\")\n",
    "        #print(x.shape[0])\n",
    "        #print(x.shape[1])\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Define the forward pass\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404739fb",
   "metadata": {},
   "source": [
    "## CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09750f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class CNNClassifier(nn.Module):\n",
    "    # n_input_data_dim1: Number of data in dimension one (number of rows/channels)\n",
    "    # n_input_data_dim2: Number of data in dimension two (number of columns/data_per_channel)\n",
    "    def __init__(self, n_input_data_dim1, n_input_data_dim2, n_output):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "           \n",
    "        # Define your layers here   \n",
    "        \n",
    "        #self.linear1 = nn.Linear(in_dim, 1024, bias=True)\n",
    "        #self.linear2 = nn.Linear(1024, 256)\n",
    "        #self.relu = nn.ReLU()\n",
    "        #self.linear3 = nn.Linear(256, out_dim)\n",
    "     \n",
    "        input_features = n_input_data_dim1 * n_input_data_dim2\n",
    "        \n",
    "        self.Conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1, \n",
    "                      out_channels = 64,\n",
    "                      kernel_size = (3, 3),\n",
    "                      stride = 1,\n",
    "                      padding = 1\n",
    "                     ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.Conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 64, \n",
    "                      out_channels = 128,\n",
    "                      kernel_size = (3, 3),\n",
    "                      stride = 1,\n",
    "                      padding = 1\n",
    "                     ),\n",
    "            nn.Dropout(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.Conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 128, \n",
    "                      out_channels = 256,\n",
    "                      kernel_size = (3, 3),\n",
    "                      stride = 1,\n",
    "                      padding = 1\n",
    "                     ),\n",
    "            nn.Dropout(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(256 * 3 * 3, n_output, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        print(\"Before the flattening\")\n",
    "        print(x.shape[0])\n",
    "        print(x.shape[1])\n",
    "        print(x.shape)\n",
    "        \n",
    "        # Define the forward pass\n",
    "        x = self.Conv1(x)\n",
    "        x = self.Conv2(x)\n",
    "        x = self.Conv3(x)\n",
    "        \n",
    "        # Perform the flattening of the input data to pass through the linear layers\n",
    "        #reshaped_sample = sample.view(sample.shape[0]*sample.shape[1])\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #x = nn.Flatten(x)\n",
    "        \n",
    "        #print(\"After the flattening\")\n",
    "        #print(x.shape[0])\n",
    "        #print(x.shape[1])\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        #return F.log_softmax(x, dim=1)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e23f61",
   "metadata": {},
   "source": [
    "# Instantiate the optimiser and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa4936",
   "metadata": {},
   "source": [
    "## Optimiser and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5381711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features: 12000\n",
      "\n",
      "torch.Size([64, 1, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([6, 2304])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Get the number of channels on the processing signal\n",
    "n_channels = len(channels)\n",
    "\n",
    "# Original number of data per channel\n",
    "n_original_data_per_channel = 60000\n",
    "\n",
    "# Get the number of data per channel\n",
    "n_data_per_channel = n_original_data_per_channel // sample_interval # Integer division\n",
    "\n",
    "#n_input_features = 2040000\n",
    "n_input_features = n_channels * n_data_per_channel # input dimension\n",
    "print(\"Number of input features: {}\\n\".format(n_input_features))\n",
    "\n",
    "# Create an instance of the neural network and move it to the device\n",
    "#net = LinearClassifier(n_channels, n_data_per_channel, n_output_classes).to(device)\n",
    "net = CNNClassifier(n_channels, n_data_per_channel, n_output_classes).to(device)\n",
    "\n",
    "#net.cuda()\n",
    "#net.cpu()\n",
    "\n",
    "# Define the loss function\n",
    "# Cross-entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Mean Square Error\n",
    "#criterion = nn.MSELoss()\n",
    "\n",
    "# Use Adam optimiser\n",
    "opt = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "#opt = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Summary of the model\n",
    "for p in net.parameters():\n",
    "    print(p.shape)\n",
    "\n",
    "# Summary of the model\n",
    "#summary(net, input_size = (batch_size, 2040000, 4))\n",
    "#summary(net, input_size = (batch_size, n_input_features))\n",
    "#summary(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd374f04",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "525b10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to generate a one-hot encoding at the position of the maximum value\n",
    "def generate_max_indices_tensor(input_tensor):\n",
    "    # Compute the maximum along each row\n",
    "    max_values, max_indices = torch.max(input_tensor, dim=1)\n",
    "    \n",
    "    # Create a tensor of zeros with the same shape as the input tensor\n",
    "    output_tensor = torch.zeros_like(input_tensor)\n",
    "    \n",
    "    # Set ones at the indices of the maximum values\n",
    "    output_tensor.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "    \n",
    "    return output_tensor\n",
    "\n",
    "# Define a helper function that returns a one tensor if the input tensors are equal\n",
    "def compare_tensors(tensor1, tensor2):\n",
    "    \n",
    "    # Ensure both tensors have the same shape\n",
    "    assert tensor1.shape == tensor2.shape, \"Both tensors should have the same dimensions.\"\n",
    "\n",
    "    # Calculate element-wise equality and count equal rows\n",
    "    row_equality = torch.all(tensor1 == tensor2, dim=1)\n",
    "    equal_rows = torch.sum(row_equality).item()\n",
    "\n",
    "    # Count different rows\n",
    "    different_rows = tensor1.shape[0] - equal_rows\n",
    "    \n",
    "    return torch.tensor([equal_rows, different_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e029e4",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f82e4c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before the flattening\n",
      "25\n",
      "2\n",
      "torch.Size([25, 2, 6000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 1, 3, 3], expected input[1, 25, 2, 6000] to have 1 channels, but got 25 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 108\u001b[0m\n\u001b[1;32m    102\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#torch.save(net.state_dict(), \"./trained_model/model.pt')\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Call the training method\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, n_epochs, loss_fn, lr)\u001b[0m\n\u001b[1;32m     28\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m batch_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#print(\"Outputs\\n\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Apply the loss function\u001b[39;00m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, batch_labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[51], line 62\u001b[0m, in \u001b[0;36mCNNClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Define the forward pass\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConv2(x)\n\u001b[1;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConv3(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 1, 3, 3], expected input[1, 25, 2, 6000] to have 1 channels, but got 25 channels instead"
     ]
    }
   ],
   "source": [
    "# Define the training method\n",
    "def train(model=net,\n",
    "    #accuracy_metric = torchmetrics.classification.Accuracy(task=\"multic\n",
    "          optimizer=opt,\n",
    "          n_epochs=n_epochs,\n",
    "          loss_fn=criterion,\n",
    "          lr=learning_rate):\n",
    "    \n",
    "    #accuracy_metric = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes = n_output_classes)\n",
    "    \n",
    "    # Indicate the Pytorch backend we are on training mode\n",
    "    model.train()\n",
    "    loss_lt = []\n",
    "    accuracy_lt = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        accuracy_epoch = 0.0\n",
    "        batch_counter = 0\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "        \n",
    "            # Prior any operation clear the gradient\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # Move data and labels to the device\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "        \n",
    "            # Forward pass\n",
    "            outputs = model(batch_data)\n",
    "\n",
    "            #print(\"Outputs\\n\")\n",
    "            #print(outputs)\n",
    "        \n",
    "            #print(\"Batch labels\\n\")\n",
    "            #print(batch_labels)\n",
    "        \n",
    "            # Apply the loss function\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "        \n",
    "            # Perform an optimization step (this updates the weights and bias on the network)\n",
    "            optimizer.step()\n",
    "                    \n",
    "            # Keep track of sum of loss of each batch\n",
    "            running_loss+=loss.item()\n",
    "            \n",
    "             # Compute the one-hot enconding version\n",
    "            one_hot_output = generate_max_indices_tensor(outputs)\n",
    "                   \n",
    "            # Compute accuracy\n",
    "            batch_accuracy = compare_tensors(one_hot_output, batch_labels)[0]\n",
    "            \n",
    "            # Keep track of accuracy\n",
    "            accuracy_epoch+=batch_accuracy\n",
    "            \n",
    "            # Metrics\n",
    "            #acc = accuracy_metric(outputs, batch_labels)\n",
    "            \n",
    "            # Print the loss for monitoring\n",
    "            print('Epoch [{}/{}], batch [{}/{}], lr={:.6f}, batch Loss: {:.4f}, batch accuracy: {:.4f}'.format(epoch+1, n_epochs,\n",
    "                                                                                                                batch_counter + 1, len(train_loader),\n",
    "                                                                                                                lr, loss.item(),\n",
    "                                                                                                                batch_accuracy*100.0/batch_size))\n",
    "            #print(\"Torch Metrics: {}\".format(acc))\n",
    "            \n",
    "            # Increase the batch counter\n",
    "            batch_counter += 1\n",
    "        \n",
    "        # Add the cumulative loss to a list\n",
    "        loss_lt.append(running_loss/len(train_loader))\n",
    "        \n",
    "        # Add the cumulative accuracy to a list\n",
    "        accuracy_lt.append(accuracy_epoch/len(train_loader)) \n",
    "        \n",
    "        # Print the total loss of the epoch\n",
    "        print('Epoch: {} training loss: {:.4f}, training accuracy: {:.4f}'.format(epoch+1, running_loss/len(train_loader), accuracy_epoch/len(train_loader)))\n",
    "        \n",
    "        #acc = accuracy_metric.compute()\n",
    "        #print(\"All batches torch metric: {}\".format(acc))\n",
    "    \n",
    "    #accuracy_metric.reset()\n",
    "    \n",
    "    fig = plt.figure(figsize=(17, 5))\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax1.plot([i for i in range(1, n_epochs+1)], loss_lt, label=\"Loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Cross-entropy\")\n",
    "    ax1.set_title(\"Training loss: optimiser {}, lr {:.6f}\".format(\"Adam\", lr))\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2 = plt.subplot(122)\n",
    "    ax2.plot([i for i in range(1, n_epochs+1)], accuracy_lt, label=\"Accuracy\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_title(\"Training accuracy: optimiser {}, lr {:.6f}\".format(\"Adam\", lr))\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "#torch.save(net.state_dict(), \"./trained_model/model.pt')\n",
    "\n",
    "# Call the training method\n",
    "train(net, opt, n_epochs, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30015c65",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing method\n",
    "def test(model=net,\n",
    "        loss_fn=criterion,\n",
    "        lr=learning_rate):\n",
    "    \n",
    "    # Indicate the Pytorch backend we are on testing mode\n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total_loss = 0.0 \n",
    "    \n",
    "    # Use no grad to reduce memory and computation cost\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        batch_counter = 0\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            \n",
    "            # Move data and labels to the device\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_data)\n",
    "            \n",
    "            #print(\"These are the outputs\")\n",
    "            #print(outputs)\n",
    "            \n",
    "            #print(\"These are the batch labels\")\n",
    "            #print(batch_labels)\n",
    "            \n",
    "            # Compute the loss\n",
    "            batch_loss = loss_fn(outputs, batch_labels)\n",
    "            \n",
    "            # Add up the loss\n",
    "            total_loss+=batch_loss.item()\n",
    "            \n",
    "            # Compute the one-hot enconding version\n",
    "            one_hot_output = generate_max_indices_tensor(outputs)\n",
    "            \n",
    "            #print(\"These are the one-hot outputs\")\n",
    "            #print(one_hot_output)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            batch_accuracy = compare_tensors(one_hot_output, batch_labels)[0]\n",
    "            #batch_accuracy = torch.sum(compare_tensors(one_hot_output, batch_labels))\n",
    "            #print(compare_tensors(one_hot_output, batch_labels))\n",
    "            \n",
    "            accuracy+=batch_accuracy\n",
    "            \n",
    "            print(\"Test batch [{}/{}], lr={:.6f},  batch loss: {:.4f}, test batch accuracy: {:.4f}\".format(\n",
    "                batch_counter + 1, len(test_loader), lr, \n",
    "                batch_loss/batch_size,\n",
    "                batch_accuracy*100.0/batch_size))\n",
    "            \n",
    "            # Increase the batch counter\n",
    "            batch_counter += 1\n",
    "            \n",
    "        print(\"Test loss: {:.4f}, test accuracy: {:.4f}\".format(\n",
    "            total_loss/(len(test_loader)*batch_size),\n",
    "            accuracy*100.0/(len(test_loader)*batch_size)))\n",
    "\n",
    "# Call the test method\n",
    "test(net, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fae8d6",
   "metadata": {},
   "source": [
    "# Code testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_data, batch_labels in train_loader:\n",
    "    print(batch_data.shape)\n",
    "    print(batch_labels)\n",
    "    print(batch_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = np.loadtxt(\"../02_python_signal_folder_sorting/sorted_signals_by_mhr/70_74/nifecg.0003.fs_1000_mhr_72_fhr_132.csv\", dtype=np.float32, delimiter=\",\")\n",
    "signal_data_torch = torch.from_numpy(signal_data)\n",
    "signal_data_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ff5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-hot enconding\n",
    "encoder = OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "print(x)\n",
    "idx, x_max = x.max(dim=1)\n",
    "print(x_max)\n",
    "x_arg_max = torch.argmax(x, 1)\n",
    "print(x_arg_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ed8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4fee4-589a-4608-bb67-0d91423ad5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
